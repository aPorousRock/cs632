{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 data not found. Did you remember to download it?\n",
      "See the comment at the top of this script.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajinkya.parkar@ibm.com/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"One-time script for extracting all the cat and dog images from CIFAR-10,\n",
    "and creating training and validation sets.\n",
    "\n",
    "Before running, download the CIFAR-10 data using these commands:\n",
    "\n",
    "$ curl -O http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "$ tar xvf cifar-10-python.tar.gz\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "if not os.path.exists('cifar-10-batches-py'):\n",
    "  print (\"CIFAR-10 data not found. Did you remember to download it?\")\n",
    "  print (\"See the comment at the top of this script.\")\n",
    "  sys.exit()\n",
    "\n",
    "TRAIN_FILES = ['cifar-10-batches-py/data_batch_%d' % i for i in range(1,6)]\n",
    "TEST_FILE = 'test_batch'\n",
    "\n",
    "CAT_INPUT_LABEL = 3\n",
    "DOG_INPUT_LABEL = 5\n",
    "\n",
    "CAT_OUTPUT_LABEL = 1\n",
    "DOG_OUTPUT_LABEL = 0\n",
    "\n",
    "def unpickle(file):\n",
    "  with open(file, 'rb') as fo:\n",
    "    dict = cPickle.load(fo, encoding='latin1')\n",
    "    return dict\n",
    "    \n",
    "data = []\n",
    "\n",
    "# Count number of cats/dogs\n",
    "num_cats = 0\n",
    "num_dogs = 0\n",
    "\n",
    "for data_file in TRAIN_FILES:\n",
    "  d = unpickle(data_file)\n",
    "  data.append(d)\n",
    "\n",
    "  for label in d['labels']:\n",
    "    if label == CAT_INPUT_LABEL:\n",
    "      num_cats += 1\n",
    "    if label == DOG_INPUT_LABEL:\n",
    "      num_dogs += 1\n",
    "    \n",
    "total = num_cats + num_dogs\n",
    "print (\"Cats :-\" ,num_cats)\n",
    "print (\"Dogs :- \" , num_dogs)\n",
    "print (\"Found %d images\" % total)\n",
    "\n",
    "# Copy the cats/dogs into new array\n",
    "images = np.empty((num_cats + num_dogs, 32, 32, 3), dtype=np.uint8)\n",
    "labels = np.empty((num_cats + num_dogs), dtype=np.uint8)\n",
    "index = 0\n",
    "#print(images)\n",
    "for data_batch in data:\n",
    "  for batch_index, label in enumerate(data_batch['labels']):\n",
    "    if label == CAT_INPUT_LABEL or label == DOG_INPUT_LABEL:\n",
    "      # Data is stored in B x 3072 format, convert to B' x 32 x 32 x 3\n",
    "      images[index, :, :, :] = np.transpose(\n",
    "          np.reshape(data_batch['data'][batch_index, :],\n",
    "          newshape=(3, 32, 32)),\n",
    "          axes=(1, 2, 0))\n",
    "      if label == CAT_INPUT_LABEL:\n",
    "        labels[index] = CAT_OUTPUT_LABEL\n",
    "      else:\n",
    "        labels[index] = DOG_OUTPUT_LABEL\n",
    "      index += 1\n",
    "    \n",
    "# split the data into train, test \n",
    "training_size = int(total * 0.8)\n",
    "print(\"Training size\", training_size)\n",
    "print(\"Testing size\", total - training_size)\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "  assert len(a) == len(b)\n",
    "  p = np.random.permutation(len(a))\n",
    "  return a[p], b[p]\n",
    "\n",
    "images, labels = unison_shuffled_copies(images, labels)\n",
    "\n",
    "train_images = images[:training_size]\n",
    "train_labels = labels[:training_size]\n",
    "\n",
    "test_images = images[training_size:]\n",
    "test_labels = labels[training_size:]\n",
    "    \n",
    "np.save('train.npy', {'images': train_images, 'labels': train_labels})\n",
    "np.save('validation.npy', {'images': test_images, 'labels': test_labels})\n",
    "\n",
    "# Make sure images look correct\n",
    "print(labels[0])\n",
    "img  = Image.fromarray(images[0, :, :, :])\n",
    "#img.show()\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              #optimizer='rmsprop',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 16\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "\n",
    "train_generator = train_datagen.flow(train_images,train_labels,batch_size=50)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = test_datagen.flow(test_images,test_labels,batch_size=20)\n",
    "'''\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=20,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=1000 // batch_size)\n",
    "model.save_weights('first_try.h5')  # always save your weights after training or during training\n",
    "'''\n",
    "\n",
    "'''\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './',  # this is the target directory\n",
    "        target_size=(32, 32),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
